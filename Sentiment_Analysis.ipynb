{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "UeY_6KCdU5q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Configs"
      ],
      "metadata": {
        "id": "HuWqiZ6mVcZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# setup device agnostic code\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ],
      "metadata": {
        "id": "KObSJWbFVcb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset (IMDB from huggingface)"
      ],
      "metadata": {
        "id": "jKfQC1XaVce4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_texts = dataset['train']['text']\n",
        "train_labels = dataset['train']['label']\n",
        "test_texts  = dataset['test']['text']\n",
        "test_labels  = dataset['test']['label']"
      ],
      "metadata": {
        "id": "rbKbsWu5Vch4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train size: {len(train_texts)}')\n",
        "print(f'Test size: {len(test_texts)}')\n",
        "train_texts[1] , train_labels[1]"
      ],
      "metadata": {
        "id": "eYaDXscNVckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "KpPQ8WMmVcnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text cleaning utility\n",
        "def clean_text(string):\n",
        "  string = string.lower()\n",
        "  string = re.sub(r\"https\\S+\",\"\",string)\n",
        "  string = re.sub(r\"@\\w+\", \"\", string)\n",
        "  string = re.sub(r\"[^a-z0-9\\s']\",\" \",string)\n",
        "  string = re.sub(r\"\\s+\",\" \",string).strip()\n",
        "\n",
        "  return string"
      ],
      "metadata": {
        "id": "gI0u39t9VcqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying utility on train-test data\n",
        "train_texts = [clean_text(t) for t in train_texts]\n",
        "test_texts = [clean_text(t) for t in test_texts]"
      ],
      "metadata": {
        "id": "qjmOsOMZVctI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization & Vocabulary"
      ],
      "metadata": {
        "id": "Rs_ryiX7YQnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(texts,min_freq = 2, max_size = 200000):\n",
        "  counter = Counter()\n",
        "  for t in texts:\n",
        "    counter.update(t.split())\n",
        "  most_common = [w for w, c in counter.most_common(max_size) if c >=min_freq]\n",
        "  itos = [\"<PAD>\",\"<OOV>\"] + most_common\n",
        "  stoi = {w:i for i , w in enumerate(itos)}\n",
        "  return stoi , itos\n",
        "\n",
        "stoi , itos = build_vocab(train_texts)\n",
        "vocab_size = len(stoi)\n",
        "print(\"Vocab Size: \", vocab_size)"
      ],
      "metadata": {
        "id": "Ya9Q0X3lYQjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def texts_to_sequence(texts,stoi):\n",
        "  seqs = []\n",
        "  for t in texts:\n",
        "    seq = [stoi.get(w,stoi[\"<OOV>\"]) for w in t.split()]\n",
        "    seqs.append(torch.tensor(seq, dtype= torch.long))\n",
        "  return seqs\n",
        "\n",
        "train_seq = texts_to_sequence(train_texts,stoi)\n",
        "test_seq = texts_to_sequence(test_texts , stoi)"
      ],
      "metadata": {
        "id": "kpu0jOmSYQfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset & DataLoader"
      ],
      "metadata": {
        "id": "gnYSIiQhYQac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self,seqs, labels):\n",
        "    self.seqs = seqs\n",
        "    self.labels = labels\n",
        "  def __len__(self):\n",
        "    return len(self.seqs)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.seqs[idx], torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "def collate_fn(batch):\n",
        "  seqs , labels = zip(*batch)\n",
        "  seqs_padded = pad_sequence(seqs, batch_first=True, padding_value= 0)\n",
        "  lengths = torch.tensor([len(s) for s in seqs])\n",
        "  labels = torch.stack(labels)\n",
        "  return seqs_padded, lengths, labels\n"
      ],
      "metadata": {
        "id": "jWW-oN8WYQXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TextDataset(train_seq,train_labels)\n",
        "test_ds = TextDataset(test_seq, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_ds,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True,\n",
        "                          collate_fn= collate_fn)\n",
        "\n",
        "test_loader = DataLoader(test_ds,\n",
        "                          batch_size=64,\n",
        "                          collate_fn= collate_fn)"
      ],
      "metadata": {
        "id": "AJbUmcbPYQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Model (LSTM)"
      ],
      "metadata": {
        "id": "nIF7cm_5YQRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim = 128, hidden_dim = 128, n_layers = 1,\n",
        "               bidirectional = True, dropout = 0.5):\n",
        "\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim , padding_idx=0)\n",
        "    self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers= n_layers,\n",
        "                        batch_first = True, bidirectional=bidirectional,\n",
        "                        dropout= dropout if n_layers > 1 else 0)\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(hidden_dim * (2 if bidirectional else 1 ), 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(64,1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x, lengths = None):\n",
        "    emb = self.emb(x)\n",
        "    out , (h,c) = self.lstm(emb)\n",
        "    h_last = torch.cat((h[-2], h[-1]), dim = 1) if self.lstm.bidirectional else h[-1]\n",
        "    logits = self.fc(h_last)\n",
        "    return logits.squeeze(1)\n"
      ],
      "metadata": {
        "id": "rjVF33-9YQOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "JdNAGB4Aq7dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training section\n",
        "\n",
        "model = RNNClassifier(vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr= 0.001)\n",
        "\n",
        "def train_one_epoch(model,dataloader):\n",
        "  model.train()\n",
        "  total_loss, preds , trues = 0, [], []\n",
        "  for X, lengths , y in tqdm(dataloader):\n",
        "    X,y = X.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X, lengths)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * X.size(0)\n",
        "    pred = (torch.sigmoid(logits) > 0.5).int().cpu().numpy()\n",
        "    preds.extend(pred)\n",
        "    trues.extend(y.cpu().numpy())\n",
        "\n",
        "  avg_loss = total_loss / len(dataloader.dataset)\n",
        "  acc = accuracy_score(trues , preds)\n",
        "  f1 = f1_score(trues, preds)\n",
        "  return acc , f1 , loss\n",
        "\n"
      ],
      "metadata": {
        "id": "vS9RPnYufGHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation section\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader):\n",
        "  model.eval()\n",
        "  preds , trues = [] , []\n",
        "  for X,lengths, y  in dataloader:\n",
        "    X , y = X.to(device), y.to(device)\n",
        "    logits = model(X, lengths)\n",
        "    pred = (torch.sigmoid(logits) > 0.5).int().cpu().numpy()\n",
        "    preds.extend(pred)\n",
        "    trues.extend(y.cpu().numpy())\n",
        "\n",
        "  acc = accuracy_score(trues, preds)\n",
        "  f1 = f1_score(trues, preds)\n",
        "  return acc , f1"
      ],
      "metadata": {
        "id": "o3qeH0Fse1NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train...!"
      ],
      "metadata": {
        "id": "Qzk6r8EKe1Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  loss, acc , f1 = train_one_epoch(model, train_loader)\n",
        "  val_acc , val_f1 = evaluate(model, test_loader)\n",
        "  print(f\"epoch {epoch+1}/{epochs} | Loss = {loss:.4f} | Train Acc: {acc:.3f} | Val Acc: {val_acc:.3f} | F1: {val_f1:.3f}\")"
      ],
      "metadata": {
        "id": "v4hETzCMe1Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}